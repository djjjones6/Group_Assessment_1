{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51fcb306-51cb-4c95-bc89-b01f9a3f99f3",
   "metadata": {},
   "source": [
    "Choosing a suitable performance metric for binary classification depends on the nature of the problem, the distribution of the classes, and the real-world implications of the predictions.\n",
    "\n",
    "\n",
    "For our bank churning dataset, the main real-world considerations for choosing a performance metric are:\n",
    "We have got an inbalanced dataset as approximately 20% exited while 80% stayed,\n",
    "Cost of False Positives vs. False Negatives,\n",
    "Threshold Optimization - Some metrics, like AUC-ROC and AUC-PR, allow you to optimize the classification threshold rather than making a hard prediction. This is useful when the business decision involves setting a threshold (e.g., setting a high threshold to avoid false positives in high-risk cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c1cfe-a52e-4ef9-ad72-555ad6356a26",
   "metadata": {},
   "source": [
    "Accuracy: Can be misleading for imbalanced datasets (e.g., predicting the majority class can give high accuracy even if the model ignores the minority class).\n",
    "\n",
    "Precision: Suitable when the cost of false positives is high. Not a priority for our scenario as not too big of a cost if we count someone who stayed as exited.\n",
    "\n",
    "Recall: Suitable when the cost of false negatives is high. Might be important for us as bigger cost when we believe that someone who has exitted has been counted as retained.\n",
    "\n",
    "F1-Score: Suitable when there is a trade-off between precision and recall, especially in imbalanced datasets. F1-score is helpful when you care equally about false positives and false negatives.\n",
    "\n",
    "ROC Curve: Plots the True Positive Rate (Recall) against the False Positive Rate (FPR = FP / (FP + TN)) for different classification thresholds.\n",
    "AUC-ROC: Measures the area under the ROC curve.\n",
    "Can be overly optimistic for highly imbalanced datasets where one class dominates.\n",
    "\n",
    "AUC-PR: Measures the area under the precision-recall curve.\n",
    "Use case: More informative than AUC-ROC for imbalanced datasets, where the focus is on the minority class. The AUC-PR curve emphasizes the model's performance with respect to the positive (minority) class.\n",
    "Limitations: Does not capture performance on negative (majority) class.\n",
    "\n",
    "Logarithmic Loss:\n",
    "Limitations: It requires probabilistic outputs and is more complex to interpret than other metrics.\n",
    "\n",
    "Matthews Correlation Coefficient (MCC)\n",
    "Use case: MCC gives a balanced measure even when the classes are imbalanced. It takes into account all four confusion matrix elements (TP, TN, FP, FN).\n",
    "Limitations: More difficult to interpret compared to precision, recall, or accuracy.\n",
    "\n",
    "Balanced Accuracy:\n",
    "Useful when the dataset is imbalanced, as it gives equal weight to the performance on both the positive and negative classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
